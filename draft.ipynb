{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e036dcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duypham/miniconda3/envs/face-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f35c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duypham/miniconda3/envs/face-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/duypham/miniconda3/envs/face-env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from losses import *\n",
    "from lr_scheduler import *\n",
    "from models.vit.vit import *\n",
    "from partial_fc_v2 import *\n",
    "from dataset import *\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transfroms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba6c6a",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "849ab792",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_src = '/home/duypham/WorkSpace/Generative_Projects/Face_Recognition/datasets/custom_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a0c1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_transforms=transfroms.Compose([\n",
    "    transfroms.RandomHorizontalFlip(),\n",
    "    transfroms.ToTensor(),\n",
    "    transfroms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
    "])\n",
    "image_transforms=transfroms.Compose([\n",
    "    transfroms.ToTensor(),\n",
    "    transfroms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9302b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 1020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3fa3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=TrainFaceRegDataset(data_src,train_image_transforms,'train',112)\n",
    "gallery_set=TrainFaceRegDataset(data_src,image_transforms,'gallery',112)\n",
    "query_set=TrainFaceRegDataset(data_src,image_transforms,'query',112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dad392",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(train_set,128,shuffle=True)\n",
    "gallery_loader=DataLoader(gallery_set,128,shuffle=False)\n",
    "query_loader=DataLoader(query_set,128,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc20a58",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b65834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (mlp): SoftMoe(\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x FeedForward(\n",
       "            (network): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (feature): Sequential(\n",
       "    (0): Linear(in_features=37632, out_features=768, bias=False)\n",
       "    (1): BatchNorm1d(768, eps=2e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (3): BatchNorm1d(768, eps=2e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "backbone = FaceNet(embed_dim=768).to(device)\n",
    "backbone.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78275dcb",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eccd5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs =50\n",
    "total_iters=len(train_dataloader)*epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df61b70",
   "metadata": {},
   "source": [
    "## loss and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16b65593",
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_loss = CombinedMarginLoss(64,1.0, 0.5, 0.00,0)\n",
    "module_partial_fc = PartialFC_V2(\n",
    "            margin_loss, 768, num_class,1,True)\n",
    "module_partial_fc.train().cuda()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=[{\"params\": backbone.parameters()}, {\"params\": module_partial_fc.parameters()}],\n",
    "    lr=1e-4, weight_decay=0.1)\n",
    "total_iters=len(train_dataloader)*epochs\n",
    "lr_scheduler = PolynomialLRWarmup(\n",
    "        optimizer=optimizer,\n",
    "        warmup_iters=total_iters//10,\n",
    "        total_iters=total_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca661b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_model import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e7e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "from eval import eval_model\n",
    "def train_loop(epochs, model,module_partial_fc, optimizer, train_dataloader,gallery_loader,query_loader, lr_scheduler,device='cuda',ckpt_path='./ckpts'):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    model.to(device)\n",
    "    module_partial_fc.to(device)\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision='fp16',\n",
    "        gradient_accumulation_steps=1,\n",
    "    )\n",
    "    model,module_partial_fc, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model,module_partial_fc, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    global_steps=0\n",
    "    # Now you train the model\n",
    "    for epoch in range(epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader))\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        module_partial_fc.train()\n",
    "        model.train()\n",
    "        for step, (image,label) in enumerate(train_dataloader):\n",
    "            global_steps+=1\n",
    "            image = image.to(device)\n",
    "            label=label.to(device)\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                img_emb = model(image)\n",
    "                loss = module_partial_fc(img_emb, label)\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_steps}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "        if epoch%50==0:\n",
    "            unwarp_model=accelerator.unwrap_model(model)\n",
    "            eval_board=eval_model(unwarp_model,gallery_loader,query_loader,device)\n",
    "            print(eval_board)\n",
    "            os.makedirs(ckpt_path,exist_ok=True)\n",
    "            torch.save(unwarp_model.state_dict(), os.path.join(ckpt_path,'model_e_{epoch}.pt'))\n",
    "            print('==>Save<==')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a873a78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "args = (epochs, backbone,module_partial_fc, optimizer, train_loader, lr_scheduler,device)\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e958d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "def visualize_tsne(model,train_loader, perplexity=30,device='cuda',save_path='./visualize_training',e=0):\n",
    "    print('Saving png file...')\n",
    "    os.makedirs(save_path,exist_ok=True)\n",
    "    new_save_path=os.path.join(save_path,str(len(os.listdir(save_path))))\n",
    "    os.makedirs(new_save_path,exist_ok=True)\n",
    "    embeddings=[]\n",
    "    labels=[]\n",
    "    model.to(device)\n",
    "    for image,label in train_loader:\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            image = image.to(device)\n",
    "            label=label.to(device)\n",
    "\n",
    "            embedding= model(image)\n",
    "            embeddings.extend(F.normalize(embedding.detach().cpu(),dim=-1).numpy().tolist())\n",
    "            labels.extend(label.detach().cpu().numpy().tolist())\n",
    "            if len(embeddings)>900:\n",
    "                break\n",
    "    X = np.array(embeddings)\n",
    "    y = np.array(labels)\n",
    "    # t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate=200)\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "    # Normalize to unit circle\n",
    "    norms = np.linalg.norm(X_tsne, ord=2, axis=1, keepdims=True)\n",
    "    X_tsne = X_tsne / norms\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"x\": X_tsne[:, 0],\n",
    "        \"y\": X_tsne[:, 1],\n",
    "        \"label\": y\n",
    "    })\n",
    "\n",
    "    # Plot with seaborn\n",
    "    plt.figure(figsize=(32, 32))\n",
    "    sns.scatterplot(\n",
    "        data=df,\n",
    "        x=\"x\", y=\"y\",\n",
    "        hue=\"label\",\n",
    "        palette=\"tab20\",\n",
    "        s=10, alpha=0.7,\n",
    "        linewidth=0\n",
    "    )\n",
    "    plt.title(\"t-SNE visualization of embeddings\")\n",
    "    # plt.savefig(os.path.join(new_save_path,f\"tsne_plot_{e}.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba69d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tsne(backbone,train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
